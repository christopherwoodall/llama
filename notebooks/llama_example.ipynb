{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4eGx5A5Qs5A",
        "outputId": "20bc3fee-20ab-45db-80a4-16f7e30e7727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "PRESIGNED_URL=\"<INSERT URL PROVIDED>\"\n",
        "TARGET_FOLDER=\"<INSERT TARGET FOLDER>\"\n",
        "\n",
        "\n",
        "if [ ! -d llama ]; then\n",
        "  mkdir \"${TARGET_FOLDER}\"\n",
        "  git clone https://github.com/facebookresearch/llama\n",
        "  pip install torch\n",
        "  pushd llama\n",
        "    pip install -r requirements.txt\n",
        "    pip install -e .\n",
        "    chmod +x download.sh\n",
        "    ./download.sh\n",
        "  popd\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import fire\n",
        "import time\n",
        "import json\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
        "\n",
        "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n"
      ],
      "metadata": {
        "id": "NSzB-2oYSG9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_parallel() -> Tuple[int, int]:\n",
        "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
        "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
        "\n",
        "    torch.distributed.init_process_group(\"nccl\")\n",
        "    initialize_model_parallel(world_size)\n",
        "    torch.cuda.set_device(local_rank)\n",
        "\n",
        "    # seed must be the same in all processes\n",
        "    torch.manual_seed(1)\n",
        "    return local_rank, world_size\n"
      ],
      "metadata": {
        "id": "n_Al5FubdkZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(ckpt_dir: str, tokenizer_path: str, local_rank: int, world_size: int) -> LLaMA:\n",
        "    start_time = time.time()\n",
        "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
        "    assert (\n",
        "        world_size == len(checkpoints)\n",
        "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
        "    ckpt_path = checkpoints[local_rank]\n",
        "    print(\"Loading\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
        "        params = json.loads(f.read())\n",
        "\n",
        "    model_args: ModelArgs = ModelArgs(max_seq_len=1024, max_batch_size=32, **params)\n",
        "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
        "    model_args.vocab_size = tokenizer.n_words\n",
        "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "    model = Transformer(model_args)\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    generator = LLaMA(model, tokenizer)\n",
        "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
        "    return \n"
      ],
      "metadata": {
        "id": "mm06pHw0dfCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(ckpt_dir: str, tokenizer_path: str, temperature: float = 0.8, top_p: float = 0.95):\n",
        "    local_rank, world_size = setup_model_parallel()\n",
        "    if local_rank > 0:\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    generator = load(ckpt_dir, tokenizer_path, local_rank, world_size)\n",
        "    prompts = [\"The capital of Germany is the city of\", \"Here is my sonnet in the style of Shakespeare about an artificial intelligence:\"]\n",
        "    results = generator.generate(prompts, max_gen_len=256, temperature=temperature, top_p=top_p)\n",
        "\n",
        "    for result in results:\n",
        "        print(result)\n",
        "        print(\"\\n==================================\\n\")\n"
      ],
      "metadata": {
        "id": "7b4MlWovdeb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fire.Fire(main)\n"
      ],
      "metadata": {
        "id": "Z2yJdqdCdot2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}